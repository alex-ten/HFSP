---
title: Discussion
category: modeling_choices
doctype: entry
entrynum: 4
---

Overall, each effect seems to be in line with our a priori expectations. However, what interests me the most is the large amount of variation in task selection left unexplained. Predicting people's actions is not trivial, even in a highly structured and constrained environment. The model shows promising results that suggests some faint traces of exploratory signals, but not more. More than half of choices could not be predicted even by the most accurate models. While the accuracy of 44% is clearly better than chance (37%), it is not enough to say that we understand what most people do most of the time. It appears that the data currently at our disposal (including the self-reported correlates of task switching) is not sufficient to make enough good guesses.

Burnham and Anderson (2002) hold that the **true model** of an underlying data-generating processes of a biological system (of which cognition is an example) has practically infinitely many parameters. This puts the modest predictive power of the best models from our candidate set into perspective. This perspective allows to better see what we assume and what is given (practically nothing). Imposing a softmax function onto a linear combination of predictors is not an inherently uncontentious assumption about the form of the **true model**. However, it seems to be viable to assume such a structure, insofar as we are willing to accept that people can *meaningfully* compute subjective values for different tasks in an active selection procedure and then compare different tasks relative to the aggregate of their values.

Each model in our all-subsets collection represents a hypothesis about the basis of task choices. Acknowledging that there might be too many different factors involved in subjective evaluations suggests that the lack of predictive accuracy of our models is either due to insufficient data, or insufficient environmental constraints. Which one is more likely depends on the goals of our investigation. If we are interested in *what* drives exploratory choices, I think it is the case that we might have missed good hypotheses (what else could explain task choices?). If we are interested specifically in whether or how "learning" (however we define it) drives exploratory choices, I think we could come up with a better study design that controls random effects and manipulates or closely observes -- and I would emphasize -- *subjective* learning. In our own monster task, having to infer such subjective feelings of learning was a major complication. I also think that there might have been unforeseen things that influenced task choices. Someone who I asked to try the task said that she just played them in the order they appeared on the choice panel (top to bottom). I also suspect that factors such as subjective boredom and fatigue could have interacted with random effects (training order, subjective liking etc.) to determine some switching decisions (especially towards the end of free play).

Suppose that for the purposes of our study we were primarily interested in if/how an operationalized measure of "learning" (e.g. `pval`) predicted the tentatively utility-based task choices. The fact that we did not find it to be a strong predictor could mean that either our operationalization was flawed or because of the reasons discussed above (e.g. perhaps somebody just liked feeding the blue bears more than some whimsical green monsters). The lack of fit could also indicate that the evaluation function has substantial individual variability, i.e. different parameterization (weights) of task features across individuals.

All of these issues seem to be empirically examinable, but some of them are more fundamental then others. For example, before we can study individual (or experimentally imposed) differences in discrete-choice models in a perfectly-controlled setup, i.e. where choices are truly made according to subjective feelings of "learnedness", we need to make sure that the very measure of this "learnedness" is properly operationalized. So far, we have tried the `pval` formulation and perhaps `pc`, and `pcr` as candidate operationalizations, but there could be other possibilities. Recognizing this leads to a realization that `pval` is itself only one of a set of candidate models of the subjective feelings of 'learnedness" (or mastery or competence). This particular model describes the relationship between the construct of subjective mastery and externally generated signal (binary feedback). More precisely, it is a model of "ignorance" of the task, since `pval` is calculated as the likelihood of the binomial distribution with known success history (number of hits, $$n$$ and number of trials, $$k$$) and fixed probability of success (= 0.5):

$$ pval = {n \choose k} 0.5^n$$

Evidently, this does not appear as a particularly faithful description of the subjective-feeling-generating process, as it requires an explicit knowledge of success history (or something like a current hit rate), or if we rewrite the likelihood function: 

$$ pval = {n \choose k} \prod_{i=1}^n 0.5^{x_i} 0.5^{1-x_i}$$

even less plausible assumptions about what needs to be represented. Although a separate study would be ideal for the formulation of the process in question, our data might have potential clues for this end. That is, even though a measure like `pval` requires an explicit representation of performance record, it might be an interesting approximation of the real process that does rely on some sort of representation about the success history. We can also compare different approximations of the same process, because we have authentic data generated by the real process. Celeste Kidd has a study of a very similar nature ({% include citation.html link='pdfs/Marti2018.pdf' cite='Marti et al., 2018'%}) where she and her colleagues compared a number of data-based performance and uncertainty measures by their goodness of fit to subjective reports of certainty of knowing the categorization rule (in the context of a Boolean category learning task). I did not find a lot of details about how they computed their uncertainty measures, but they claim to have found good evidence that an operationalization (of subjective certainty of knowing a rule in such a task) based on performance (e.g. hit rate over last 5 trials) is better than abstract uncertainty measures (e.g. entropy, however it was computed). I think our data permits a replication of that finding (to some extent).

We could try to select a good operationalization out of a set of candidate models by fitting each one to `lrn` ratings ("Rate each monster family based on how much more you think you could learn if you had more time to play with it ..."). Here, we must assume that learnability is inversely proportional to "learnedness" as if the more one thinks one can learn about a task, the less that task is judged to be learned. One potential complication with this approach is the ambiguity of the question and the resulting ambiguity of the self-reports. Suppose the subject has a chance-level performance and a `pval` not low enough judge a task learned. They could either rate the learnability of that task close to 1 ("Definitely could NOT learn more") if they are certain that the task is unlearnable, or 10 ("Definitely could learn more") if they think they can still learn something about it. The `pval` is unreliable in these situations and it is not be obvious whether it is because of poor operationalization (in `pval`) or poor instrumentation or validity (in `lrn` ratings).

Alternatively, we could use subjective competence judgments more directly through progress ratings, `prog` ("Rate each monster family based on how much progress you felt you made for learning their food preferences ..."). We must assume that these progress ratings are generated by comparing subjective competence judgments across some period of experience with a task in question. The best model of these competence ratings should be the one that fits the difference between competence judgments at the beginning and at the end of the session. We can measure that fit by calculating the residual:

$$ 
    \epsilon = y - \hat y \\\\
    = y - (g(x_{end}) - g(x_{beg}))

$$

where $$ y $$ is the actual progress rating provided by the subject, and $$ \hat y =  $$ is the difference between competence, $$ g(x) $$ at the end of the session and competence at the beginning of the session. The computation of the residual requires similar scaling between $$y$$ and $$\hat y$$. This can be achieved by restricting the objectively measurable progress $$ g(x_{end}) - g(x_{beg}) $$ to a range between $$[0, 1]$$. We can do this by replacing all negative progress values with 0 (interpreting it as "no progress") and min-max normalizing the resulting data to a range between $$[0, 1-g(x_{beg})]$$. Subjective ratings can be similarly normalized to span the interval between $$[0,1]$$ and the values can be compared directly by a simple subtraction. A summary index of the residuals across subjects and tasks can serve as the basis for comparing various forms of $$ g(x) $$. The input to the competence function will depend on the function form. It could be, for example, a series of raw feedback, hit rate, sensitivity, or something else.