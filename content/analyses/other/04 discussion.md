---
title: Discussion
category: open_loop
doctype: entry
entrynum: 4
---

Burnham and Anderson (2002) believe that the **true model** of an underlying data-generating processes of a biological system (which I think includes cognition) has practically infinitely many parameters. This puts the modest predictive power of the best models from our candidate set into perspective. Imposing a softmax function onto a linear combination of predictors is not an inherently uncontentious assumption about the form of the **true model**. However, it seems to be viable to assume that, insofar as we are willing to accept that people can meaningfully compute subjective values for task selection. For the purposes of our study we were primarily interested if/how an operationalized measure of "learning" (e.g. `pval`) predicted utility-based task selection. The fact that we did not find it to be a strong predictor could mean that either our operationalization was flawed or that there was a lack of constraints on the subjective evaluation of tasks (perhaps somebody just liked the blue bears more than green monsters). The lack of fit could also indicate that the evaluation function has substantial individual variability, i.e. different parameterization (weights) of task features across individuals.

All of these issues seem to be empirically examinable, but some of them are more fundamental then others. For example, before we can study individual (or experimentally imposed) differences in discrete-choice models in a perfectly-controlled setup, i.e. where choices are made according to subjective "learnedness", we need to make sure that the very measure of this "learnedness" is properly operationalized. So far, we have tried the `pval` formulation and perhaps `pc`, and `rpc` as candidate operationalizations, but there could be other possibilities. Recognizing this leads to a realization that `pval` is itself only one of a set of candidate models of the subjective feelings of 'learnedness" (or mastery or competence). This particular model describes the relationship between the construct of subjective mastery and externally generated signal (binary feedback). More precisely, it is a model of "ignorance" of the task, since `pval` is calculated as the likelihood of the binomial distribution with known success history (number of hits, $$n$$ and number of trials, $$k$$) and fixed probability of success (= 0.5):

$$ pval = {n \choose k} 0.5^n$$

Evidently, this is not a particularly faithful description of the subjective-feeling-generating process, as it requires an explicit knowledge of success history, or if we rewrite the likelihood function: 

$$ pval = {n \choose k} \prod_{i=1}^n 0.5^{x_i} 0.5^{1-x_i}$$

even less plausible assumptions about what needs to be represented. Although a separate study would be ideal for the formulation of the process in question, our data might have potential clues for this end. That is, even though a measure like `pval` requires an explicit representation of performance record, it might be an interesting approximation of the real process that does rely on some sort of representation about the success history. That means that even if we don't know how to find the "true" model, we can always approximate the process in question. We can also compare different approximations of the same process, because we have authentic data generated by the real process. 

We could try to select the "best" model out of a set of candidate models by fitting each one to `lrn` ratings ("Rate each monster family based on how much more you think you could learn if you had more time to play with it ..."). Here, we must assume that learnability is inversely proportional to "learnedness" as if the more one thinks one can learn about a task, the less that task is judged to be learned. One potential complication with this approach is the ambiguity of the question and the resulting ambiguity of the self-reports. Suppose the subject has a chance-level performance and a `pval` not low enough judge a task learned. They might rate the learnability of that task closer to 10 ("Definitely could learn more") if they are certain that the task is unlearnable, or 1 ("Definitely could NOT learn more") if they think they can still learn something about it. The `pval` would provide a poor fit in this situation and it would not be obvious whether it is because of poor operationalization (in `pval`) or poor instrumentation (in `lrn` ratings).

Alternatively, we could use subjective competence judgments more directly through progress ratings, `prog` ("Rate each monster family based on how much progress you felt you made for learning their food preferences ..."). We must assume that these progress ratings are generated by comparing subjective competence judgments across some period of experience with a task in question. The best model of these competence ratings should be the one that fits the difference between competence judgments at the beginning and at the end of the session. We can measure that fit by calculating the residual:

$$ 
    \epsilon = y - \hat y \\\\
    = y - (g(x_{end}) - g(x_{beg}))

$$

where $$ y $$ is the actual progress rating provided by a subject, and $$ \hat y =  $$ is the difference between competence, $$ g(x) $$ at the end of the session and competence at the beginning of the session. The computation of the residual requires similar scaling between $$y$$ and $$\hat y$$. This can be achieved by restricting the objectively measurable progress $$ g(x_{end}) - g(x_{beg}) $$ to the range between $$[0, 1]$$. We can do this by replacing all negative progress values to 0 and min-max normalizing the resulting data that lies between $$[0, 1-g(x_{beg})]$$. Subjective ratings can be similarly normalized to span the interval between $$[0,1]$$ and the values can be compared directly by a simple subtraction. A summary index of the residuals across subjects and tasks can serve as the basis for comparing various forms of $$ g(x) $$.